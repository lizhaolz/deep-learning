# -*- coding: utf-8 -*-
"""
Created on Mon Aug  6 10:47:54 2018

@author: 10943
"""

import numpy as np
import matplotlib.pyplot as plt
import sklearn
import sklearn.datasets
import sklearn.linear_model
from testCases import *
from planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets
np.random.seed(1) # set a seed so that the results are consistent

#load data
X, Y = load_planar_dataset() 

#Visualize the dataset using matplotlib
#plt.scatter(X[0, :], X[1, :],c=np.squeeze(Y),s=40, cmap=plt.cm.Spectral);

#get the shape of X and Y,and get the number of sample

X_shape=X.shape
Y_shape=Y.shape
m=X_shape[1]
#print(X_shape)
#print(Y_shape)
#print(m)
#use sklearn's built-in functions to train a logistic regression classifier on the dataset.
clf=sklearn.linear_model.LogisticRegressionCV()
clf.fit(X.T, Y.T)

# Plot the decision boundary for logistic regression
#plot_decision_boundary(lambda x: clf.predict(x), X, Y)
#plt.title("Logistic Regression")
# Print accuracy
LR_predictions = clf.predict(X.T)
#print ('Accuracy of logistic regression: %d ' % float((np.dot(Y,LR_predictions) + np.dot(1-Y,1-LR_predictions))/float(Y.size)*100) +
#       '% ' + "(percentage of correctly labelled datapoints)")

#define variables
# n_x: the size of input layer
# n_h: the size of hidden layer
# n_y: the size of output layer
n_x=X_shape[0]
n_h=4
n_y=Y_shape[0]

#Initialize the model's parameters
def initialize_parameters(n_x,n_h, n_y):
    W1=np.random.randn(n_h,n_x)*0.01
    b1=np.zeros((n_h,1))
    W2=np.random.randn(n_y,n_h)*0.01
    b2=np.zeros((n_y,1))
    assert(W1.shape==(n_h,n_x))
    assert(b1.shape==(n_h,1))
    assert(W2.shape==(n_y,n_h))
    assert(b2.shape==(n_y,1))
    parameters={"W1":W1,
                "b1":b1,
                "W2":W2,
                "b2":b2}
    return parameters


#parameters = initialize_parameters(n_x, n_h, n_y)
#print("W1 = " + str(parameters["w1"]))
#print("b1 = " + str(parameters["b1"]))
#print("W2 = " + str(parameters["w2"]))
#print("b2 = " + str(parameters["b2"])) 

#forward propagation
def forward_propagation(X,parameters):
    W1=parameters['W1']
    b1=parameters['b1']
    W2=parameters['W2']
    b2=parameters['b2']
    Z1= np.dot(W1,X)+b1
    A1=np.tanh(Z1)
    Z2=np.dot(W2,A1)+b2
    A2=sigmoid(Z2)
    assert(A2.shape == (1, X.shape[1]))
    cache = {"Z1": Z1,
             "A1": A1,
             "Z2": Z2,
             "A2": A2}
    
    return A2, cache

#A2, cache=forward_propagation(X,parameters)
#print(cache['Z1'].shape)
#print(np.mean(cache['Z1']) ,np.mean(cache['A1']),np.mean(cache['Z2']),np.mean(cache['A2']))

#compute cost
def compute_cost(cache,Y):
    logprobs=np.multiply(np.log(cache['A2']),Y)+np.multiply(np.log((1-cache['A2'])),(1-Y))
    cost=-np.sum(logprobs)
    cost=np.squeeze(cost)
    cost/=m
    return cost

#cost=compute_cost(cache,Y)
#print(cost)

#backward propagation
def backward_propagation(parameters,cache,X,Y):
    m=X.shape[1]
    
    W1=parameters['W1']
    W2=parameters['W2']
    
    A1=cache['A1']
    A2=cache['A2']
    
    dZ2=A2-Y
    dW2=np.dot(dZ2/m,A1.T)
    db2=np.sum(dZ2,axis=1,keepdims=True)
    dZ1=np.dot(W2.T,dZ2)*(1-np.power(A1,2))
    dW1=np.dot(dZ1/m,X.T)
    db1=np.sum(dZ1,axis=1,keepdims=True)
    grads = {"dW1": dW1,
             "db1": db1,
             "dW2": dW2,
             "db2": db2}
    
    return grads
#grads = backward_propagation(parameters, cache, X, Y)
#print ("dW1 = "+ str(grads["dW1"]))
#print ("db1 = "+ str(grads["db1"]))
#print ("dW2 = "+ str(grads["dW2"]))
#print ("db2 = "+ str(grads["db2"]))

#update parameters
def update_parameters(parameters,grads,learning_rate=0.01):
    W1=parameters['W1']
    b1=parameters['b1']
    W2=parameters['W2']
    b2=parameters['b2']
    
    dW1=grads['dW1']
    db1=grads['db1']
    dW2=grads['dW2']
    db2=grads['db2']
    
    W1-=learning_rate*dW1
    b1-=learning_rate*db1
    W2-=learning_rate*dW2
    b2-=learning_rate*db2
    
    parameters = {"W1": W1,
                  "b1": b1,
                  "W2": W2,
                  "b2": b2}
    
    return parameters

#parameters = update_parameters(parameters, grads)

#print("W1 = " + str(parameters["W1"]))
#print("b1 = " + str(parameters["b1"]))
#print("W2 = " + str(parameters["W2"]))
#print("b2 = " + str(parameters["b2"]))

#build neural network modle
def nn_modle(X,Y,n_h,num_iterations = 10000, print_cost=False):
    
    """
    Arguments:
    X -- dataset of shape (2, number of examples)
    Y -- labels of shape (1, number of examples)
    n_h -- size of the hidden layer
    num_iterations -- Number of iterations in gradient descent loop
    print_cost -- if True, print the cost every 1000 iterations
    
    Returns:
    parameters -- parameters learnt by the model. They can then be used to predict.
    """
    n_x=X.shape[0]
    n_y=Y.shape[0]
    
    
    # Initialize parameters
    parameters = initialize_parameters(n_x, n_h, n_y)

    # loop(Gradient descent)
    for i in range(0,num_iterations):
        # forward propaggation
    
        A2,cache=forward_propagation(X,parameters)
        
        #cost function
        cost=compute_cost(cache,Y)
        
        #backward propagation
        grads = backward_propagation(parameters, cache, X, Y)
        
        # Gradient descent parameters update
        parameters = update_parameters(parameters, grads)
        
        # print the cost every 1000 iterations
        if print_cost and i % 1000 == 0:
            print ("Cost after iteration %i: %f" %(i, cost))
        
    return parameters

parameters = nn_modle(X, Y, 4, num_iterations=10000, print_cost=True)
print("W1 = " + str(parameters["W1"]))
print("b1 = " + str(parameters["b1"]))
print("W2 = " + str(parameters["W2"]))
print("b2 = " + str(parameters["b2"]))

# predict
def predict(parameters,X):
    A2,cache=forward_propagation(X,parameters)
    prediction=A2
    return prediction

predictions = predict(parameters, X)
print("predictions mean = " + str(np.mean(predictions)))
